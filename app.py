from PIL import Image
from transformers import BlipProcessor, BlipForConditionalGeneration, pipeline
import gradio as gr


# 1Ô∏è‚É£ Mod√®le BLIP (Captioning en anglais)
processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")

# 2Ô∏è‚É£ Pipelines de traduction
# Anglais ‚Üí Arabe
translator_en_ar = pipeline("translation", model="Helsinki-NLP/opus-mt-en-ar")
# Anglais ‚Üí Fran√ßais
translator_en_fr = pipeline("translation", model="Helsinki-NLP/opus-mt-en-fr")




def generate_caption(img, use_detailed=False):
    """G√©n√®re une description (caption) en anglais via BLIP"""
    img_input = Image.fromarray(img)
    inputs = processor(img_input, return_tensors="pt")

    params = {}
    if use_detailed:
        params = {
            "max_length": 80,
            "min_length": 20,
            "num_beams": 5,
            "repetition_penalty": 1.2,
            "length_penalty": 1.5,
            "temperature": 0.7,
        }

    out = model.generate(**inputs, **params)
    caption = processor.decode(out[0], skip_special_tokens=True)
    return caption


def translate_text(text, target_pipeline):
    """Traduit le texte anglais dans une autre langue avec le pipeline sp√©cifi√©"""
    try:
        translated = target_pipeline(text, max_length=512)
        return translated[0]["translation_text"]
    except Exception as e:
        return f"Erreur de traduction : {e}"


def process_image(img, detail_level):
    """Retourne la description en anglais, en fran√ßais et en arabe"""
    use_detailed = detail_level == "D√©taill√©e"

    # 1Ô∏è‚É£ Description en anglais
    caption_en = generate_caption(img, use_detailed=use_detailed)

    # 2Ô∏è‚É£ Traduction en fran√ßais
    caption_fr = translate_text(caption_en, translator_en_fr)

    # 3Ô∏è‚É£ Traduction en arabe
    caption_ar = translate_text(caption_en, translator_en_ar)

    return caption_en, caption_fr, caption_ar




demo = gr.Interface(
    fn=process_image,
    inputs=[
        gr.Image(label="Importez une image √† analyser"),
        gr.Radio(
            choices=["Simple", "D√©taill√©e"],
            value="D√©taill√©e",
            label="Niveau de d√©tail de la description"
        ),
    ],
    outputs=[
        gr.Text(label="üá¨üáß 1. Description (Anglais)"),
        gr.Text(label="üá´üá∑ 2. Description (Fran√ßais)"),
        gr.Text(label="üåô 3. ÿßŸÑŸàÿµŸÅ (Arabe)", rtl=True),
    ],
    title="Image Captioning Multilingue (EN ‚Üí FR ‚Üí AR)",
    description="T√©l√©versez une image pour obtenir automatiquement une description en anglais, fran√ßais et arabe.",
    allow_flagging="never"
)


if __name__ == "__main__":
    # host="0.0.0.0" pour Render / HuggingFace / Docker
    import os
    demo.launch(server_name="0.0.0.0",server_port=int(os.environ.get("PORT", 7860)))

